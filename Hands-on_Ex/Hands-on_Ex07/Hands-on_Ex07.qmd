---
title: "Hands-on Exercise 7"
execute: 
  warning: false
date: "`r Sys.Date()`"
highlight-style: dracula
---

## **Geographical Segmentation with Spatially Constrained Clustering Techniques**

## **1. Overview**

In this hands-on exercise, we will learn how to delineate homogeneous regions by using geographically referenced multivariate data. There are two major analysis, namely:

-   hierarchical cluster analysis; and
-   spatially constrained cluster analysis.

In this exercise, we are interested to delineate the Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.

## **2. The Data**

-   Myanmar Township Boundary Data. This is GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar.
-   Shan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.

## **3. Installing and Loading R packages**

The R packages needed for this exercise are as follows:

**Spatial data handling :** sf, rgdal and spdep

**Attribute data handling :** tidyverse, especially readr, ggplot2 and dplyr

**Choropleth mapping :** tmap

**Multivariate data visualisation and analysis :** coorplot, ggpubr, and heatmaply

**Cluster analysis :** cluster, ClustGeo

```{r}
pacman::p_load(sp, rgdal, spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

## **4. Data Import and Prepatation**

### Importing geospatial data into R environment

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))
```

Viewing the content:

```{r}
shan_sf
```

```{r}
glimpse(shan_sf)
```

### Importing aspatial data into R environment

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
```

Summary statistics:

```{r}
summary(ict)
```

### Derive new variables using dplyr package

The measurement unit for the values is the number of households. Utilizing raw values from the aspatial data directly may introduce bias due to the underlying total number of households.

To overcome this problem, we will derive the penetration rate of each ICT variable by using the code below.

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

Summary statistics of the newly derived penetration rates:

```{r}
summary(ict_derived)
```

::: callout-note
Notice that six new fields have been added into the data.frame.
:::

## **5. Exploratory Data Analysis (EDA)**

### EDA using statistical graphics

Histogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Boxplot is useful to detect if there are outliers.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

We will also plot the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code below.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

Below are the other histograms plotted to reveal the distribution of the selected variables in the ict_derived data.frame.

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

The code below groups the above histograms together:

```{r}
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

### EDA using choropleth map

Before we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. 

```{r}
shan_sf <- left_join(shan_sf, 
                     ict_derived, by=c("TS_PCODE"="TS_PCODE"))
  
write_rds(shan_sf, "data/shan_sf.rds")
```

```{r}
shan_sf <- read_rds("data/shan_sf.rds")
```

Preparing the map:

```{r}
qtm(shan_sf, "RADIO_PR")
```

In order to reveal that the distribution shown in the choropleth map above are biased to the underlying total number of households at the townships, we will create two choropleth maps. One will be for the total number of households (i.e. TT_HOUSEHOLDS.map) and the other will be for the total number of household with Radio (RADIO.map).

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

::: callout-note
Notice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.
:::

Now we plot the choropleth maps showing the distribution of total number of households and radio penetration rate.

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

## **6. Correlation Analysis**

In this section, we learn how to use corrplot.mixed() which is a function of the corrplot package to visualise and analyse the correlation of the input variables.

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

The correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.

## **7. Hierarchy Cluster Analysis**

### Extracting clustering variables

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

::: callout-note
Notice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.
:::

Next, we need to change the rows by township name instead of row number.

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

::: callout-note
Notice that the row number has been replaced into the township name.
:::

Now, we will delete the TS.x field.

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

### Data Standardisation

In general, multiple variables will be used in a cluster analysis. It is not unusual that the range of their values are different. 

In order to avoid a cluster analysis result that is biased to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.

### Min-Max standardisation

In the code below, the function normalize() of the heatmaply package is used to stadardisation the clustering variables by using Min-Max method. 

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

::: callout-note
Notice that the range of values of the clustering variables after min-max standardissation are 0-1 now.
:::

### Z-score standardisation

Z-score standardisation can be performed easily by using scale() of Base R.

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

::: callout-note
Notice that the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.
:::

::: callout-warning
Z-score standardisation method should only be used if we assume all variables come from some normal distribution.
:::

### Visualising the standardised clustering variables

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

### Computing proximity matrix

 We will compute the proximity matrix by using dist() of R.

dist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
```

Listing the contents:

```{r}
proxmat
```

### Computing hierarchical clustering

In R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.

hclust() employs the agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

Plot:

```{r}
plot(hclust_ward, cex = 0.6)
```

### Selecting the optimal clustering algorithm

One of the challenges faced in performing hierarchical clustering is identifying stronger clustering structures. This issue can be solved by using the agnes() function of the cluster package.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

With reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.

### Determining Optimal Clusters

Another technical challenge faced in performing clustering analysis is determining the optimal clusters to retain.

here are three commonly used methods to determine the optimal clusters, they are:

-   Elbow Method
-   Average Silhouette Method
-   Gap Statistic Method

In this exercise, we will be exploring the Gap Statistic Method.

The idea behind the gap statistic is that if the clustering structure in the original dataset is meaningful, the inertia will be significantly lower than the average inertia of random datasets. The method helps to avoid overfitting (using too many clusters) or underfitting (using too few clusters) in clustering analysis.

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

Plot:

```{r}
fviz_gap_stat(gap_stat)
```

With reference to the gap statistic graph above, the recommended number of clusters to retain is 1. However, it is not logical to retain only one cluster. By examining the graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.

::: callout-note
The NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.
:::

### Interpreting the dendrograms

In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are.

::: callout-note
We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.
:::

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

### Visually-driven hierarchical clustering analysis

With the heatmaply package, we are able to build both highly interactive cluster heatmaps or static cluster heatmaps.

The data that was loaded was initially a data frame, but it has to be a data matrix in order to make the heatmap.

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

Building the heatmap:

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

### Mapping the clusters formed

After close examination of the dendrogram above, we have decided to retain six clusters.

cutree() of R Base will be used in the code below to derive a 6-cluster model.

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
# The output is called groups. It is a list object.
```

In order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

The choropleth map above reveals the clusters are very fragmented. The is one of the major limitations when non-spatial clustering algorithms such as hierarchical cluster analysis method is used.

## **8. Spatially Constrained Clustering: SKATER approach**

### Converting into SpatialPolygonsDataFrame

First, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because the SKATER function only supports sp objects such as SpatialPolygonDataFrame.

```{r}
shan_sp <- as_Spatial(shan_sf)
```

### Computing Neighbour List

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

Plot: The first plot command gives the boundaries. This is followed by the plot of the neighbour list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.

```{r}
plot(shan_sp, 
     border=grey(0.5))
plot(shan.nb, 
     coordinates(shan_sp), 
     col="blue", 
     add=TRUE)
```

::: callout-note
If you plot the network first and then the boundaries, some of the areas will be clipped. 
:::

### Computing minimum spanning tree

Calculate edge costs:

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

Next, we incorporate these costs into a weights object in the same way as the calculation of inversed distance weights. In other words, we convert the neighbour list to a list weights object by specifying the newly computed lcosts as the weights.

::: callout-note
Note that we specify the style as B to make sure the cost values are not row-standardised.
:::

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

The minimum spanning tree is computed by with the mstree() function of the spdep package.

```{r}
shan.mst <- mstree(shan.w)
```

Checking class and dimension:

```{r}
class(shan.mst)
```

```{r}
dim(shan.mst)
```

Note that the dimension is 54 and not 55. This is because the minimum spanning tree consists of n-1 edges (links) in order to traverse all the nodes.

Displaying content:

```{r}
head(shan.mst)
```

Plot:

```{r}
plot(shan_sp, border=gray(.5))
plot.mst(shan.mst, 
         coordinates(shan_sp), 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

### Computing spatially constrained clusters using SKATER method

The skater() takes three mandatory arguments: 
-   the first two columns of the MST matrix (i.e. not the cost)
-   the data matrix (to update the costs as units are being grouped), and 
-   the number of cuts. 

::: callout-note
It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.
:::

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

Display contents:

```{r}
str(clust6)
```

Check cluster assignment:

```{r}
ccs6 <- clust6$groups
ccs6
```

We can find out how many observations are in each cluster by means of the table command.

```{r}
table(ccs6)
```

For example, the first list has a node with dimension 22, which is also the number of observations in the first cluster.

Plot:

```{r}
plot(shan_sp, border=gray(.5))
plot(clust6, 
     coordinates(shan_sp), 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

### Visualising the clusters in choropleth map

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

For easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```

## **9. Spatially Constrained Clustering: ClustGeo Method**

ClustGeo package is an R package specially designed to support spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.

### Ward-like hierarchical clustering: ClustGeo

To perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix.

```{r}
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

::: callout-note
Note that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist()
:::

Similarly, we can plot the clusters on a categorical area shaded map.

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))
```

```{r}
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

```{r}
qtm(shan_sf_ngeo_cluster, "CLUSTER")
```

### Spatially Constrained Hierarchical Clustering

Before we can perform spatially constrained hierarchical clustering, a spatial distance matrix must be derived by using st_distance().

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
```

Next, choicealpha() will be used to determine a suitable value for the mixing parameter alpha.

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```

With reference to the graphs above, alpha = 0.3 will be used.

```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.3)
```

```{r}
# derive cluster object
groups <- as.factor(cutree(clustG, k=6))
```

```{r}
# bind the group list with shan_sf polygon feature data frame
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

```{r}
# plot
qtm(shan_sf_Gcluster, "CLUSTER")
```

## **10. Visual Interpretation of Clusters**

### Visualising individual clustering variable

```{r}
ggplot(data = shan_sf_ngeo_cluster,
       aes(x = CLUSTER, y = RADIO_PR)) +
  geom_boxplot()
```

The boxplot reveals Cluster 3 displays the highest mean for Radio Ownership Per Thousand Household.

### Multivariate Visualisation

```{r}
ggparcoord(data = shan_sf_ngeo_cluster, 
           columns = c(17:21), 
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE, 
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster") +
  facet_grid(~ CLUSTER) + 
  theme(axis.text.x = element_text(angle = 30))
```

The parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.

Lastly, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.

```{r}
shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(CLUSTER) %>%
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```
